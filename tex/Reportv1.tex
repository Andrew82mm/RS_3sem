\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{indentfirst}

\geometry{
	a4paper,
	left=30mm,
	right=15mm,
	top=20mm,
	bottom=20mm
}

% Настройка гиперссылок
\hypersetup{
	colorlinks=true,
	linkcolor=black,
	citecolor=black,
	filecolor=black,
	urlcolor=blue
}

% Переименование заголовков
\addto\captionsrussian{
	\renewcommand{\contentsname}{Оглавление}
	\renewcommand{\refname}{Список литературы}
}

% Настройка отступов
\setlength{\parindent}{1.25cm}
\setlength{\parskip}{0pt}

\begin{document}
	
	% ============ ТИТУЛЬНЫЙ ЛИСТ ============
	\begin{titlepage}
		\centering
		
		{\large САНКТ-ПЕТЕРБУРГСКИЙ ГОСУДАРСТВЕННЫЙ УНИВЕРСИТЕТ\par}
		\vspace{0.5cm}
		{\large Искусственный интеллект и наука о данных\par}
		
		\vspace{4cm}
		
		{\large Сергиенко Андрей\par}
		
		\vspace{2cm}
		
		{\Large\bfseries Применение алгоритмов K-ближайших соседей в коллаборативных рекомендательных системах\par}
		
		\vspace{0.5cm}
		
		{\large Отчёт о прохождении\\Учебной (ознакомительной) практики\par}
		
		\vfill
		
		\begin{flushright}
			Научный руководитель:\\
			Старший преподаватель кафедры системного программирования\\
			Юрий Александрович Андреев
		\end{flushright}
		
		\vfill
		
		{\large Санкт-Петербург\par}
		{\large 2025\par}
		
	\end{titlepage}
	
	% ============ ОГЛАВЛЕНИЕ ============
	\newpage
	\tableofcontents
	\newpage
	
	% ============ ВВЕДЕНИЕ ============
	\section*{Введение}
	\addcontentsline{toc}{section}{Введение}
	
	Современные информационные системы ежедневно обрабатывают огромные объёмы данных, среди которых особое место занимают персонализированные рекомендации~--- фильмы, музыка, товары, новости и многое другое. Рекомендательные системы позволяют пользователям находить интересный контент, а компаниям~--- повышать вовлечённость и продажи.
	
	Одним из наиболее популярных подходов к построению рекомендательных систем является коллаборативная фильтрация (Collaborative Filtering, CF), основанная на идее, что пользователи с похожими интересами будут оценивать похожие объекты схожим образом. Ключевой задачей коллаборативной фильтрации является поиск похожих пользователей или объектов, для чего широко применяется алгоритм K-ближайших соседей (K-Nearest Neighbors, KNN).
	
	Однако точный поиск ближайших соседей имеют высокую вычислительную сложность и становится неэффективным при работе с большими наборами данных. Поэтому активно развиваются приближённые методы поиска ближайших соседей (Approximate Nearest Neighbors, ANN), такие как Annoy, FAISS и HNSW, обеспечивающие компромисс между скоростью и точностью.
	
	Настоящая работа посвящена исследованию и сравнению эффективности различных реализаций алгоритма K-ближайших соседей в контексте коллаборативных рекомендательных систем.
	
	\newpage
	
	% ============ ПОСТАНОВКА ЗАДАЧИ ============
	\section{Постановка задачи}
	
	Цель работы~--- исследовать эффективность различных алгоритмов K-ближайших соседей для реализации коллаборативной рекомендательной системы, а также провести сравнительный анализ их производительности, точности и использования памяти.
	
	Для достижения цели необходимо решить следующие задачи:
	
	\begin{enumerate}
		\item Изучить основные принципы работы коллаборативной фильтрации и алгоритма K-ближайших соседей.
		\item Реализовать базовую рекомендательную систему на основе пользовательских и предметных матриц.
		\item Реализовать и сравнить четыре метода поиска ближайших соседей:
		\begin{itemize}
			\item Точный KNN (scikit-learn);
			\item Annoy (Spotify);
			\item FAISS (Meta);
			\item HNSW (Hierarchical Navigable Small World).
		\end{itemize}
		\item Провести измерения времени построения индекса, скорости запросов, точности (Recall@20, Precision@20) и использования памяти.
		\item Визуализировать результаты и сформулировать выводы о применимости каждого метода.
	\end{enumerate}
	
	\newpage
	
	% ============ ОБЗОР ============
	\section{Обзор}
	
	\subsection{Обзор рекомендательных систем и их классификация}
	
	Рекомендательные системы (RS) представляют собой программные системы, задача которых~--- предсказать интерес пользователя к объекту на основе анализа исторических данных. Существует три основных подхода к построению RS:
	
	\begin{enumerate}
		\item \textbf{Контентная фильтрация} (Content-Based Filtering)~--- анализирует характеристики объектов и строит рекомендации, исходя из сходства контента.
		\item \textbf{Коллаборативная фильтрация} (Collaborative Filtering, CF)~--- основывается на взаимодействиях пользователей и объектов (рейтинги, покупки, клики).
		\item \textbf{Гибридные методы} (Hybrid Methods)~--- объединяют оба подхода для повышения качества рекомендаций.
	\end{enumerate}
	
	Коллаборативная фильтрация, в свою очередь, делится на:
	\begin{itemize}
		\item \textbf{User-based CF}~--- рекомендации формируются на основе схожих пользователей;
		\item \textbf{Item-based CF}~--- рекомендации формируются на основе схожих объектов.
	\end{itemize}
	
	\subsection{Алгоритм K-ближайших соседей и его применение}
	
	Алгоритм K-ближайших соседей (KNN) является одним из базовых методов в коллаборативной фильтрации. Он позволяет находить элементы, наиболее похожие на заданный, по метрике сходства (например, косинусное сходство или корреляция Пирсона).
	
	В контексте рекомендательных систем:
	\begin{itemize}
		\item В user-based CF ищутся пользователи, схожие по поведению с данным пользователем;
		\item В item-based CF~--- объекты, схожие с теми, что пользователь уже оценил.
	\end{itemize}
	
	Рекомендации формируются как взвешенное усреднение оценок ближайших соседей, что позволяет эффективно предсказывать предпочтения даже при отсутствии контентной информации.
	
	\subsection{Проблемы масштабируемости и приближённые методы}
	
	Основным недостатком точного KNN является высокая вычислительная сложность.
	
	Пусть у нас:
	\begin{itemize}
		\item $N$~--- количество объектов в базе
		\item $Q$~--- количество запросов
		\item $d$~--- размерность поискового пространства
		\item $K$~--- количество искомых соседей
	\end{itemize}
	
	Вычисление расстояния между двумя векторами требует $O(d)$.\\
	Так как нужно сравнить со всеми $N$ точками, то $O(N \cdot d)$.\\
	Для всех $Q$ запросов~--- $O(Q \cdot N \cdot d)$.\\
	Используем алгоритм Quickselect~--- $O(N + K \log K)$.\\
	После вычисления всех $N$ расстояний нужно выбрать $K$ наименьших~--- $O(Q \cdot (N \cdot d + K \log K))$.
	
	При росте числа пользователей, запросов и объектов (например, в системах с миллионами записей) такой подход становится непрактичным.
	
	Для решения этой проблемы используются Approximate Nearest Neighbors (ANN)~--- приближённые методы поиска, которые жертвуют небольшой долей точности ради значительного выигрыша в скорости.
	
	ANN-методы строят специальные структуры данных (деревья, графы, хеши), что позволяет находить близкие элементы за логарифмическое или даже константное время.
	
	\subsection{Существующие библиотеки поиска ближайших соседей}
	
	Среди наиболее популярных библиотек для реализации ANN можно выделить:
	
	\begin{itemize}
		\item \textbf{Annoy (Spotify)}~--- метод случайных проекций и построения деревьев, эффективен для больших векторов признаков.
		\item \textbf{FAISS (Meta)}~--- оптимизирована под большие данные и вычисления на GPU, активно используется в индустриальных решениях.
		\item \textbf{HNSW (Hierarchical Navigable Small World)}~--- графовая структура данных, обеспечивающая высокую точность и малое время отклика даже при миллионах элементов.
	\end{itemize}
	
	\subsection{Ожидаемые результаты}
	
	В ходе работы будет реализован программный комплекс для сравнения точного и приближённых алгоритмов поиска ближайших соседей в коллаборативных системах рекомендаций.
	
	Ожидается, что:
	\begin{itemize}
		\item Точный KNN обеспечит наилучшую точность (Recall@20, Precision@20), но низкую производительность на больших данных;
		\item Annoy покажет компромисс между скоростью и точностью, потребляя умеренные ресурсы;
		\item FAISS продемонстрирует высокую скорость при больших объёмах данных, особенно с GPU;
		\item HNSW обеспечит оптимальный баланс между скоростью, точностью и памятью, являясь лучшим выбором для production-систем.
	\end{itemize}
	
	Результаты исследования будут представлены в виде таблиц и графиков, отражающих:
	\begin{itemize}
		\item время построения индекса;
		\item среднее время запроса;
		\item использование оперативной памяти;
		\item показатели точности (Precision@20, Recall@20).
	\end{itemize}
	
	\newpage
	
	% ============ ПРОЕКТИРОВАНИЕ ============
	\section{Проектирование}
	
	\subsection{Архитектура системы} 
	
	Архитектура разработанной системы для генерации эмбеддингов рекомендательной системы построена по модульному принципу и состоит из следующих основных компонентов: 
	
	\textbf{1. Модуль загрузки и предварительной обработки данных}
	\begin{itemize}
		\item Загрузка датасета MovieLens (файлы movies.csv и ratings.csv).
		\item Удаление избыточных признаков (жанры фильмов, временные метки) для концентрации на данных взаимодействий.
		\item Первичная валидация и проверка целостности данных.
	\end{itemize} 
	
	\textbf{2. Модуль фильтрации и индексации данных}
	\begin{itemize}
		\item Отбор активных пользователей (оставлены пользователи с более чем 50 оценок).
		\item Отбор популярных фильмов (оставлены фильмы с более чем 10 оценками).
		\item Создание непрерывных отображений (маппингов) для идентификаторов пользователей и фильмов в индексы матрицы, что позволяет избежать «пробелов» в индексации.
	\end{itemize} 
	
	\textbf{3. Модуль формирования разреженной матрицы взаимодействий}
	\begin{itemize}
		\item Прямое формирование разреженной матрицы «пользователь-фильм» в формате CSR (Compressed Sparse Row) из отфильтрованных данных.
		\item Использование векторов оценок, индексов пользователей и фильмов для эффективного построения матрицы без создания промежуточных плотных структур.
	\end{itemize} 
	
	\textbf{4. Модуль матричной факторизации и генерации эмбеддингов}
	\begin{itemize}
		\item Применение алгоритма TruncatedSVD из библиотеки Scikit-learn для выполнения матричной факторизации.
		\item Снижение размерности исходной разреженной матрицы до латентного пространства заданной размерности (128 компонентов).
		\item Генерация векторных представлений (эмбеддингов) для пользователей и фильмов как результатов разложения матрицы.
	\end{itemize} 
	
	\textbf{5. Модуль сохранения результатов и модели}
	\begin{itemize}
		\item Сериализация полученных эмбеддингов в удобные для использования форматы (CSV для интерпретации, NumPy для быстрой загрузки).
		\item Сохранение созданных маппингов ID в индексы для последующего использования в сервисе рекомендаций.
		\item Сохранение обученной модели SVD для возможности ее дальнейшего использования или дообучения.
	\end{itemize} 
	
	\newpage
	
	\textbf{6. Модуль реализации алгоритмов поиска}
	\begin{itemize}
		\item Единый интерфейс для всех алгоритмов с методами build(), query\_user(), query\_item()
		\item Четыре независимые реализации: ExactKNN, AnnoyKNN, FAISSKNN, HNSWKNN
		\item Поддержка как user-based, так и item-based подходов
	\end{itemize}
	
	\textbf{7. Модуль мониторинга производительности}
	\begin{itemize}
		\item Класс MemoryTracker для измерения потребления оперативной памяти
		\item Замеры времени построения индексов и выполнения запросов
		\item Использование библиотеки psutil для точного профилирования
	\end{itemize}
	
	\textbf{8. Модуль оценки точности}
	\begin{itemize}
		\item Генерация тестовой выборки из 100 случайных запросов
		\item Сравнение результатов приближённых методов с эталонным Exact KNN
		\item Вычисление метрик Recall@20 и Precision@20
	\end{itemize}
	
	\textbf{9. Модуль визуализации и анализа}
	\begin{itemize}
		\item Построение шести сравнительных графиков
		\item Сохранение результатов в CSV-формате
		\item Генерация текстового отчёта с рекомендациями
	\end{itemize}
	
	Взаимодействие между модулями организовано последовательно: данные проходят через этапы загрузки, обработки, построения индексов, тестирования и визуализации результатов.
	
	\subsection{Выбор и подготовка датасета}
	
	В качестве экспериментального датасета был выбран MovieLens(32m)~--- один из наиболее распространённых наборов данных для исследований в области рекомендательных систем. Датасет содержит:
	\begin{itemize}
		\item Файл movies.csv: идентификаторы фильмов, названия и жанры.
		\item Файл ratings.csv: оценки пользователей (userId, movieId, rating, timestamp).
	\end{itemize}
	
	Преимущества выбранного датасета:
	\begin{itemize}
		\item Реальные данные от пользователей сервиса MovieLens, что обеспечивает высокую внешнюю валидность результатов.
		\item Высокая разреженность, типичная для коллаборативных систем и представляющая ключевую вычислительную проблему.
	\end{itemize}
	
	На этапе предварительной обработки были выполнены следующие операции:
	
	\textbf{Удаление избыточных признаков}: столбцы `genres` и `timestamp` были исключены, так как в данной работе исследуется исключительно коллаборативная фильтрация без использования контентных признаков.
	
	\textbf{Фильтрация пользователей и объектов}: для повышения качества рекомендаций и уменьшения вычислительной сложности были отобраны только активные пользователи (с числом оценок $> 50$) и популярные фильмы (с числом оценок $> 10$). Это позволило снизить влияние шумовых взаимодействий и сосредоточиться на объектах с достаточным количеством информации для построения надёжных моделей. В результате было выделено 126 588 пользователей и 30 521 фильм.
	
	\subsection{Формирование пользовательско-предметной матрицы}
	
	Центральным элементом коллаборативной фильтрации является матрица взаимодействий <<пользователь--объект>>. В данной работе была сформирована матрица размерностью $126\,588 \times 30\,521$, где строки соответствуют пользователям, столбцы~--- фильмам, а значения~--- оценкам от 0.5 до 5.0. Показатель разреженности матрицы составил 99.24\% (плотность 0.76\%).
	
	\textbf{Проблема плотного представления}
	
	Теоретически можно построить плотную матрицу размера $126\,588 \times 30\,521$, заполнив отсутствующие оценки нулями или специальными значениями. Однако такой подход имеет ряд критических недостатков:
	\begin{itemize}
		\item \textbf{Чрезмерные требования к памяти}: плотная матрица такого размера содержит более $3.86$ млрд элементов. При хранении в формате \texttt{float32} это потребовало бы порядка 15.5~ГБ памяти, что значительно превышает объём оперативной памяти большинства рабочих станций.
		\item \textbf{Шум из-за заполнения пропусков}: подавляющее большинство взаимодействий отсутствует, и заполнение нулями создаёт искусственные связи <<пользователь не любит этот фильм>>, хотя отсутствие оценки не означает отрицательное отношение.
		\item \textbf{Неэффективные вычисления}: большая часть операций (умножение матриц, вычисление расстояний) будет выполняться на бесполезных нулевых элементах.
	\end{itemize}
	
	\textbf{Преобразование в разреженный формат}
	
	Для эффективной работы с высокоразреженными данными матрица была конвертирована в формат CSR (Compressed Sparse Row) с использованием библиотеки SciPy. Формат CSR хранит только ненулевые элементы и их индексы, что обеспечивает:
	\begin{itemize}
		\item Существенную экономию памяти (в данном случае хранится лишь 0.76\% от полного объёма).
		\item Быструю индексацию строк и эффективное выполнение матричных операций, критически важных для последующих этапов.
	\end{itemize}
	
	\textbf{Почему разреженного формата недостаточно}
	
	Несмотря на существенное снижение объёма данных, разреженная матрица остаётся крайне высокой размерности (десятки тысяч признаков на пользователя). Это приводит к следующим ограничениям:
	\begin{itemize}
		\item \textbf{Проблема <<проклятия размерности>>}: вычисление расстояний или сходства между разреженными векторами с десятками тысяч измерений становится неинформативным --- большинство пар пользователей не имеют пересечения по оценённым фильмам.
		\item \textbf{Невозможность обобщения}: если два пользователя не оценили общие фильмы, методы на основе расстояний не смогут определить их сходство.
		\item \textbf{Высокие вычислительные затраты}: операции со столь широкими матрицами требуют значительных ресурсов, даже при хранении их в CSR-формате.
	\end{itemize}
	
	Эти ограничения делают невозможным эффективное использование разреженной матрицы напрямую для поиска соседей или построения рекомендаций. Поэтому применяется следующий этап: снижение размерности и генерация эмбеддингов.
	
	\subsection{Снижение размерности и генерация эмбеддингов}
	
	Чтобы получить компактное и информативное представление пользователей и фильмов, был применён подход матричной факторизации, позволяющий перейти от разреженного пространства исходных оценок к плотным низкоразмерным векторным представлениям~--- эмбеддингам.
	
	\textbf{Концепция эмбеддингов}. Эмбеддинг представляет собой плотный вектор фиксированного размера (в данном случае 128), который кодирует сущность (пользователя или фильм) в непрерывном векторном пространстве. В отличие от разреженного представления, эмбеддинги не просто указывают на наличие взаимодействия, а захватывают его \textit{латентные (скрытые) признаки}. Для фильма это могут быть абстрактные характеристики, такие как <<уровень драматизма>>, <<научно-фантастичность>> или <<динамичность экшена>>. Для пользователя эмбеддинг отражает его предпочтения по этим же латентным осям.
	
	\textbf{Преимущества использования эмбеддингов}:
	\begin{itemize}
		\item \textbf{Снижение размерности}: Переход от пространства размерностью $30\,521$ (число фильмов) к пространству размерностью 128 значительно упрощает вычисления.
		\item \textbf{Вычислительная эффективность}: Расчёт метрик сходства в 128-мерном пространстве на порядки быстрее, чем в исходном разреженном пространстве.
		\item \textbf{Борьба с разреженностью}: Плотные векторы позволяют определять сходство даже между пользователями, не имеющими совместно оценённых фильмов.
		\item \textbf{Обобщение предпочтений}: Модель улавливает скрытые паттерны в поведении пользователей и структуре фильмов, сглаживая шум и исключая влияние единичных оценок.
		\item \textbf{Сжатие информации}: Эмбеддинги обеспечивают уменьшение числа признаков более чем в 200 раз и при этом сохраняют значимую часть дисперсии данных, что подтверждает эффективность кодирования.
	\end{itemize}
	
	\textbf{Технология реализации}. Для снижения размерности матрицы взаимодействий был использован метод усечённого сингулярного разложения (\texttt{TruncatedSVD}) из библиотеки \texttt{scikit-learn}. Классическое сингулярное разложение (SVD) представляется формулой
	\[
	R = U \Sigma V^T.
	\]
	
	Такое разложение можно понимать как разбиение исходной матрицы $R$ на набор \emph{основных направлений}, по которым данные изменяются сильнее всего. Эти направления задаются столбцами матриц $U$ и~$V$, а величины изменений вдоль них~— значениями диагональной матрицы $\Sigma$.
	
	Каждое сингулярное значение показывает, насколько ``важна'' соответствующая компонента: чем оно больше, тем больший вклад эта компонента вносит в структуру данных. Если оставить только $k$ наибольших сингулярных значений, то мы получим приближение
	\[
	R \approx U_k \Sigma_k V_k^T,
	\]
	которое содержит только самые информативные латентные признаки.
	
	Этот подход является оптимальным в следующем смысле: приближение ранга $k$, полученное таким образом, сохраняет \emph{максимально возможную} часть информации исходной матрицы среди всех возможных матриц ранга $k$. Поэтому такой метод хорошо подходит для задач, где нужно уменьшить размерность без существенной потери смысла данных.
	
	Метод \texttt{TruncatedSVD} эффективно работает с разреженными матрицами и не требует преобразования в плотный формат. В итоге в данной работе были получены:
	\begin{itemize}
		\item матрица эмбеддингов пользователей $U_k$ размером $(126\,588, 128)$;
		\item матрица эмбеддингов фильмов $V_k$ размером $(30\,521, 128)$.
	\end{itemize}
	
	Каждая из 128 координат в этих эмбеддингах соответствует одному из главных скрытых признаков, которые SVD сумело выделить из исходных оценок.
	
	
	Полученные векторные представления являются конечным продуктом этапа предварительной подготовки и служат основой для построения моделей рекомендаций, основанных на вычислении сходства векторов в общем латентном пространстве.
	
	\subsection*{Вывод по результатам генерации эмбеддингов}
	
	В результате факторизации пользовательско-предметной матрицы методом усечённого сингулярного разложения (SVD) были получены плотные эмбеддинги пользователей и фильмов размерностью 128. Несмотря на крайне высокую разреженность исходной матрицы (0.76\% заполненности), метод позволил выделить ключевые латентные структуры данных, сохранив 42.07\% общей дисперсии оценок.
	
	Созданные векторы имеют компактный размер: 153.43~МБ вместо 14.7~ГБ, которые потребовались бы для хранения плотной матрицы, что соответствует сжатию информации в 96 раз без существенной потери качества. Полученные эмбеддинги успешно используются для вычисления сходства между пользователями и фильмами в низкоразмерном пространстве и демонстрируют корректность рекомендаций на практике: для тестового пользователя были возвращены фильмы с ожидаемо высокими оценками и жанровой близостью.
	
	Показатель «42\% объяснённой дисперсии» означает, что выбранные 128 латентных признаков сохраняют 42\% всей информации, содержащейся в исходной разреженной матрице оценок. Несмотря на то, что это значение меньше 100\%, его оказывается достаточно для построения рекомендаций. Это объясняется тем, что в реальных данных большая часть сигналов относится к шуму: оценки пользователей неоднородны, субъективны и часто неполны. Латентные компоненты SVD автоматически отбрасывают шумовые направления и сохраняют только наиболее устойчивые и значимые зависимости между пользователями и фильмами.
	
	Таким образом, применение SVD позволило эффективно преобразовать исходные данные в информативное и вычислительно удобное представление, являющееся надёжной основой для построения рекомендательной системы.
	
	\newpage
	
	\section{Реализация}
	
	\subsection{Архитектура системы}
	Все методы поиска ближайших соседей реализованы как наследники единого абстрактного базового класса, обеспечивающего унифицированный интерфейс для построения индексов и выполнения запросов. Такой подход гарантирует консистентность измерений и упрощает добавление новых алгоритмов.
	
	Для обеспечения корректного измерения потребления памяти применена архитектура изолированных процессов, состоящая из двух типов компонентов: orchestrator (координатор) и worker (исполнитель).
	\textbf{Orchestrator-процесс} выполняет следующие функции:
	\begin{enumerate}
		\item Подготовка общих данных: загрузка эмбеддингов, генерация тестовой выборки из 100 случайных пользователей и фильмов (seed=42 для воспроизводимости)
		\item Вычисление ground truth с использованием ExactKNN для всех тестовых запросов
		\item Сериализация подготовленных данных (тестовые индексы и ground truth) во временные pickle-файлы для передачи worker-процессам
		\item Последовательный запуск изолированных worker-процессов для каждого алгоритма через модуль subprocess
		\item Сбор результатов из stdout каждого worker-процесса путем парсинга JSON-данных между маркерами \texttt{\_\_RESULT\_START\_\_} и \texttt{\_\_RESULT\_END\_\_}
		\item Агрегация результатов в pandas DataFrame и генерация итоговых отчетов
		\item Очистка временных файлов после завершения всех измерений
	\end{enumerate}
	\textbf{Worker-процесс} работает в полностью изолированном окружении:
	\begin{enumerate}
		\item Загрузка собственной копии эмбеддингов в чистое адресное пространство процесса
		\item Десериализация тестовых индексов и ground truth из временных файлов
		\item Инициализация конкретного алгоритма с оптимизированными параметрами из конфигурационного файла
		\item Построение индекса с одновременным отслеживанием потребления памяти через класс MemoryTracker (использует psutil для мониторинга RSS)
		\item Выполнение тестовых запросов с измерением времени и вычислением метрик точности
		\item Сериализация результатов в JSON и вывод в stdout для захвата orchestrator-процессом
	\end{enumerate}
	Такая архитектура полностью исключает влияние накладных расходов других методов, артефактов сборщика мусора Python и фрагментации памяти. Каждый worker-процесс завершается сразу после отправки результатов, освобождая все выделенные ресурсы. Это позволяет получить точные изолированные измерения потребления памяти для каждого алгоритма.
	
	\subsection{Оптимизация гиперпараметров}
	
	Перед финальным тестированием для всех приближённых методов была проведена процедура настройки гиперпараметров методом grid search. 
	Целью оптимизации является нахождение баланса между точностью и скоростью выполнения запросов.
	
	\subsubsection{Методология настройки}
	
	Для объективной оценки качества каждой комбинации параметров была разработана композитная метрика:
	
	\[
	\text{Score} = 0.4 \cdot \text{Recall@20} 
	- 0.6 \cdot \ln\!\left( 1 + \text{QueryTime}_{\text{ms}} \right).
	\]
	
	Эта формула балансирует два ключевых требования:
	\begin{itemize}
		\item \textbf{Точность} (вес 0.4): более высокий Recall@20 увеличивает итоговый скор;
		\item \textbf{Скорость} (вес 0.6): логарифм времени запроса вычитается, поэтому более быстрые алгоритмы получают преимущество.
	\end{itemize}
	
	Использование логарифма времени позволяет корректно обрабатывать различия в несколько порядков и избежать доминирования временной компоненты над точностью. 
	Веса 0.4/0.6 были выбраны эмпирически с приоритетом на производительность, что соответствует требованиям production-систем рекомендаций.
	
	\subsubsection{Пространство поиска параметров}
	
	Для каждого алгоритма определена сетка гиперпараметров, основанная на рекомендациях разработчиков библиотек и предварительных экспериментах.
	
	\textbf{Annoy:}
	\begin{itemize}
		\item \texttt{n\_trees} $\in \{25, 50, 100, 200\}$ --- количество деревьев в лесу.
	\end{itemize}
	
	\textbf{FAISS:}
	\begin{itemize}
		\item \texttt{nlist} $\in \{100, 400, 800, 1600\}$ --- количество кластеров для IVF-индекса.
	\end{itemize}
	
	\textbf{HNSW:}
	\begin{itemize}
		\item \texttt{ef\_construction} $\in \{200, 400, 800\}$ --- размер динамического списка при построении;
		\item \texttt{M} $\in \{16, 32, 48\}$ --- количество двунаправленных связей на элемент;
		\item Общее количество комбинаций: $3 \times 3 = 9$.
	\end{itemize}
	
	\subsubsection{Результаты настройки}
	
	Процедура grid search выполнена на той же тестовой выборке из 100 запросов, что использовалась для финального benchmarking. 
	Для каждой комбинации параметров вычислены Recall@20, среднее время запроса и композитный скор.
	
	\textbf{Annoy} --- протестировано 4 конфигурации:
	\begin{itemize}
		\item Лучшая конфигурация: \texttt{n\_trees=50}, Score = 0.199;
		\item Увеличение количества деревьев до 200 повышает точность с 0.76 до 0.95, 
		но увеличивает время запросов с 0.127 мс до 0.669 мс, что снижает композитный скор;
		\item Оптимум смещён к минимальному значению \texttt{n\_trees} из-за высокого веса скорости.
	\end{itemize}
	
	\textbf{FAISS} --- протестировано 4 конфигурации:
	\begin{itemize}
		\item Лучшая конфигурация: \texttt{nlist=1600}, Score = 0.291;
		\item \texttt{nlist=800} обеспечивает баланс: точность 0.966 при времени 0.249 мс;
		\item \texttt{nlist=1600} увеличивает точность до 0.99, но замедляет запросы до 0.510 мс.
	\end{itemize}
	
	\textbf{HNSW} --- протестировано 9 комбинаций:
	\begin{itemize}
		\item Лучшая конфигурация: \texttt{ef\_construction=200}, \texttt{M=16}, Score = 0.327;
		\item Параметр \texttt{M=16} показал стабильно высокие результаты во всех настройках;
		\item Конфигурация (200, 16) достигла точности 0.9735 при времени запроса 0.11 мс;
		\item Максимальная точность 0.994 (800, 48) достигается за счёт увеличения времени до 0.201 мс, что снижает итоговый скор до 0.201.
	\end{itemize}
	
	Все результаты настройки сохранены в файл \texttt{tuning/tuning\_results.csv} для последующего анализа. 
	Найденные оптимальные параметры зафиксированы в конфигурационном файле \texttt{tuning/best\_params.json} и использованы для финального тестирования приближённых методов.
	
	
	\subsection{Реализация точного KNN}
	
	Точный метод K-ближайших соседей был реализован с использованием класса NearestNeighbors из библиотеки scikit-learn. Данная реализация служит эталоном (ground truth) для оценки точности приближённых методов.
	
	\textbf{Принцип работы}: Метод полного перебора (brute-force) вычисляет расстояния от запроса до всех точек в наборе данных и возвращает k ближайших. Этот подход гарантирует нахождение точных ближайших соседей, но требует больших вычислительных ресурсов при большом размере данных.
	
	\textbf{Ключевые параметры:}
	\begin{itemize}
		\item \texttt{metric='cosine'}~--- косинусное расстояние, наиболее подходящее для сравнения векторов оценок
		\item \texttt{algorithm='brute'}~--- полный перебор всех элементов для гарантии точности
		\item \texttt{n\_neighbors=20}~--- количество возвращаемых ближайших соседей
		\item \texttt{n\_jobs=-1}~--- использование всех доступных процессорных ядер
	\end{itemize}
	
	\textbf{Результаты построения индексов:}
	\begin{itemize}
		\item Время построения user-based индекса: 0.0056 с
		\item Время построения item-based индекса: 0.0015 с
		\item Потребление памяти: 0.00 MB (измеримых накладных расходов не обнаружено)
	\end{itemize}
	
	Быстрое построение индекса объясняется отсутствием предварительной обработки~--- метод brute-force не создаёт дополнительных структур данных, а работает напрямую с исходной матрицей эмбеддингов.
	
	\textbf{Производительность запросов:}
	\begin{itemize}
		\item Среднее время запроса user-based: 139.59 мс
		\item Среднее время запроса item-based: 29.36 мс
	\end{itemize}
	Как видно из результатов, время выполнения запроса в десятки тысяч раз превышает время построения индекса, что является характерной особенностью точного метода. Для каждого запроса вычисляются расстояния до всех пользователей (или фильмов), что приводит к линейной зависимости времени от размера датасета. Различие во времени между user-based и item-based запросами объясняется разным размером пространств (количеством пользователей и фильмов).
	
	\textbf{Точность:}
	\begin{itemize}
		\item Recall@20: 1.0000 (user-based и item-based)
		\item Precision@20: 1.0000 (user-based и item-based)
	\end{itemize}
	
	По определению, точный метод возвращает идеально корректные результаты, что подтверждается максимальными значениями метрик.
	
	\subsection{Реализация приближённых методов поиска (Annoy, FAISS, HNSW)}
	
	\subsubsection{Annoy (Approximate Nearest Neighbors Oh Yeah)}
	
	Annoy~--- библиотека от Spotify, основанная на построении леса случайных проекционных деревьев.
	
	\textbf{Принцип работы}: Для каждого дерева выбирается случайная гиперплоскость, разделяющая пространство на две части. Процесс рекурсивно повторяется, формируя бинарное дерево. При поиске запрос проходит по всем деревьям, и результаты объединяются. Этот метод позволяет быстро находить приближенные ближайшие соседи за счет уменьшения пространства поиска.
	
	\textbf{Параметры реализации:}
	\begin{itemize}
		\item \texttt{n\_trees=50}~--- количество деревьев в лесу
		\item \texttt{metric='angular'}~--- эквивалент косинусного расстояния
	\end{itemize}
	
	\textbf{Результаты:}
	\begin{itemize}
		\item Время построения user-based: 2.78 с
		\item Время построения item-based: 0.43 с
		\item Потребление памяти: 314.74 MB
		\item Среднее время запроса user-based: 0.19 мс
		\item Среднее время запроса item-based: 0.14 мс
		\item Recall@20 user-based: 0.760
		\item Recall@20 item-based: 0.922
		\item Precision@20 user-based: 0.760
		\item Precision@20 item-based: 0.922
	\end{itemize}
	
	\textbf{Анализ}: Annoy продемонстрировал наибольшее ускорение запросов (в $\sim$734 раза быстрее Exact KNN для user-based, в $\sim$209 раз для item-based) при умеренном снижении точности. Однако метод показал наибольшее потребление памяти среди всех приближённых алгоритмов (314.74 MB), что связано с хранением множественных деревьев. Точность user-based поиска (76.0\%) существенно ниже, чем у конкурентов, что может быть критичным для некоторых применений.
	
	\subsubsection{FAISS (Facebook AI Similarity Search)}
	
	FAISS~--- библиотека от Meta, оптимизированная для работы с большими объёмами данных и GPU-ускорением.
	
	\textbf{Принцип работы}: Используется метод IVF (Inverted File Index)~--- пространство разбивается на кластеры (ячейки Вороного), и при поиске проверяются только ближайшие кластеры. Этот подход значительно уменьшает количество вычислений, так как не нужно проверять все точки в пространстве.
	
	\textbf{Параметры реализации:}
	\begin{itemize}
		\item \texttt{nlist=1600}~--- целевое количество кластеров
		\item \texttt{nprobe=20}~--- количество кластеров, проверяемых при поиске
		\item L2-нормализация векторов для корректного вычисления косинусного сходства
	\end{itemize}
	
	\textbf{Результаты:}
	\begin{itemize}
		\item Время построения user-based: 8.54 с
		\item Время построения item-based: 2.04 с
		\item Потребление памяти: 131.54 MB
		\item Среднее время запроса user-based: 0.18 мс
		\item Среднее время запроса item-based: 0.06 мс
		\item Recall@20 user-based: 0.939
		\item Recall@20 item-based: 0.985
		\item Precision@20 user-based: 0.939
		\item Precision@20 item-based: 0.985
	\end{itemize}
	
	\textbf{Анализ}: FAISS показал хороший баланс между скоростью и точностью. Запросы выполняются в $\sim$775 раз быстрее Exact KNN для user-based и в $\sim$489 раз для item-based, при этом достигнута высокая точность: 93.9\% для user-based и 98.5\% для item-based. Умеренное потребление памяти (131.54 MB) и стабильно высокая точность делают FAISS эффективным выбором для production-систем, требующих надёжного компромисса между всеми характеристиками.
	
	\subsubsection{HNSW (Hierarchical Navigable Small World)}
	
	HNSW~--- графовый алгоритм, строящий многослойную структуру связей между элементами.
	
	\textbf{Принцип работы}: Создаётся иерархия графов, где верхние уровни содержат разреженные длинные связи для быстрой навигации, а нижние~--- плотные локальные связи для точного поиска. Этот подход напоминает принцип работы дорожных сетей, где скоростные магистрали (верхние уровни) позволяют быстро перемещаться на большие расстояния, а местные дороги (нижние уровни) обеспечивают точное достижение пункта назначения.
	
	\textbf{Параметры реализации:}
	\begin{itemize}
		\item \texttt{ef\_construction=200}~--- размер динамического списка при построении
		\item \texttt{M=16}~--- количество двунаправленных связей на элемент
	\end{itemize}
	
	\textbf{Результаты:}
	\begin{itemize}
		\item Время построения user-based: 7.27 с
		\item Время построения item-based: 0.45 с
		\item Потребление памяти: 121.05 MB
		\item Среднее время запроса user-based: 0.12 мс
		\item Среднее время запроса item-based: 0.06 мс
		\item Recall@20 user-based: 0.973
		\item Recall@20 item-based: 0.998
		\item Precision@20 user-based: 0.973
		\item Precision@20 item-based: 0.998
	\end{itemize}
	\textbf{Анализ}: HNSW продемонстрировал выдающийся баланс между скоростью и точностью. Метод обеспечивает максимальное ускорение запросов среди всех протестированных алгоритмов (в $\sim$1163 раза быстрее Exact KNN для user-based, в $\sim$489 раз для item-based) при наивысшей точности: 97.3\% и 99.8\% соответственно~--- фактически достигнута точность, близкая к эталонной. При этом HNSW демонстрирует минимальное потребление памяти среди приближённых методов (121.05 MB). Это подтверждает теоретические преимущества графового подхода для задач поиска ближайших соседей и делает HNSW оптимальным выбором для высоконагруженных систем.
	
	\subsection{Методика оценки точности и производительности}
	Для объективного сравнения алгоритмов была разработана комплексная система оценки, включающая четыре группы метрик:
	
	\subsubsection{Метрики производительности}
	\textbf{1. Время построения индекса}~--- измеряется для каждого алгоритма отдельно для user-based и item-based подходов. Отражает накладные расходы на предварительную обработку данных и создание вспомогательных структур.
	
	\textbf{2. Время выполнения запроса}~--- усредняется по 100 случайным тестовым запросам. Ключевая метрика для production-систем, где требуется обработка запросов в реальном времени.
	
	\textbf{3. Потребление оперативной памяти}~--- измеряется с использованием архитектуры изолированных процессов. Каждый алгоритм запускается в отдельном subprocess через модуль subprocess, что позволяет точно измерить приращение памяти без влияния артефактов других методов и сборщика мусора Python. Важно для систем с ограниченными ресурсами.
	
	\subsubsection{Метрики точности}
	Для оценки качества приближённых методов используется сравнение с результатами Exact KNN:
	\textbf{Recall@20}~--- доля правильно найденных соседей из топ-20:
	
	$\text{Recall@20} = \frac{|\text{найденные соседи} \cap \text{истинные соседи}|}{|\text{истинные соседи}|}$
	
	\textbf{Precision@20}~--- доля релевантных объектов среди возвращённых:
	
	$\text{Precision@20} = \frac{|\text{найденные соседи} \cap \text{истинные соседи}|}{|\text{найденные соседи}|}$
	
	В данном случае, так как оба множества содержат ровно 20 элементов, Recall@20 = Precision@20
	
	\subsubsection{Процедура тестирования}
	\begin{enumerate}
		\item Генерация тестовой выборки из 100 случайных пользователей и 100 случайных фильмов (seed=42 для воспроизводимости)
		\item Получение эталонных результатов от Exact KNN для всех тестовых запросов
		\item Сохранение ground truth и тестовых индексов во временные файлы для передачи worker-процессам
		\item Последовательный запуск каждого алгоритма в изолированном subprocess
		\item Выполнение тех же запросов для каждого приближённого метода в чистом окружении
		\item Попарное сравнение результатов с использованием множественных операций
		\item Вычисление средних значений метрик по всем тестовым запросам
		\item Сбор результатов через JSON-сериализацию в stdout worker-процессов
	\end{enumerate}
	
	\subsubsection{Визуализация результатов}
	
	Результаты представлены в виде шести графиков:
	\begin{enumerate}
		\item Время построения индекса~--- сравнение накладных расходов
		\item Потребление памяти~--- оценка ресурсоёмкости
		\item Среднее время запроса (логарифмическая шкала)~--- демонстрация различий в производительности
		\item Recall@20~--- оценка полноты результатов
		\item Precision@20~--- оценка точности результатов
		\item Компромисс скорость-точность~--- scatter-plot для выявления оптимальных алгоритмов
	\end{enumerate}
	
	Такой комплексный подход позволяет оценить каждый алгоритм с разных сторон и выбрать оптимальное решение в зависимости от приоритетов конкретной задачи.
	
	\newpage
	
	% ============ ЗАКЛЮЧЕНИЕ ============
	\section{Заключение}
	
	В рамках данной работы было проведено исследование эффективности различных реализаций алгоритма K-ближайших соседей для коллаборативной рекомендательной системы. Реализованы и протестированы четыре метода: точный KNN (scikit-learn), Annoy, FAISS и HNSW на датасете MovieLens.
	
	\textbf{Основные результаты исследования:}
	
	\textbf{Производительность построения индексов:}
	\begin{itemize}
		\item ---
	\end{itemize}
	
	\textbf{Производительность запросов:}
	\begin{itemize}
		\item ---
	\end{itemize}
	
	\textbf{Точность рекомендаций:}
	\begin{itemize}
		\item ---
	\end{itemize}
	
	\textbf{Потребление памяти:}
	\begin{itemize}
		\item ---
	\end{itemize}
	
	\newpage
	
	\textbf{Выводы:}
	
	Экспериментальные результаты подтвердили теоретические преимущества приближённых методов поиска ближайших соседей. ТУТ НАПИШУ КАКОЙ МЕТОД ЛУЧШЕ ВСЕХ И В КАКИХ СЛУЧАЯХ СТОИТ ИСПОЛЬЗОВАТЬ ОСТАЛЬНЫЕ
	
	Разработанная система сравнения может быть использована для тестирования других алгоритмов и датасетов, а полученные результаты служат основой для выбора оптимального метода в зависимости от конкретных требований к производительности и точности рекомендательной системы.
	
	
	% ============ СПИСОК ЛИТЕРАТУРЫ ============
	\newpage
	\begin{thebibliography}{99}
		\addcontentsline{toc}{section}{Список литературы}
		
		\bibitem{movielens}
		MovieLens Dataset. GroupLens Research. URL: https://grouplens.org/datasets/movielens/
		
		
		
	\end{thebibliography}
	
\end{document}